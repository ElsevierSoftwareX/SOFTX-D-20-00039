

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python Package genieclust Reference &mdash; genieclust 0.9.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="genieclust: Fast and Robust Hierarchical Clustering with Noise Point Detection" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> genieclust
          

          
          </a>

          
            
            
              <div class="version">
                0.9.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python Package <cite>genieclust</cite> Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#genie">Genie</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gic">GIc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-genieclust.compare_partitions">External Cluster Validity Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-genieclust.inequity">Inequity Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-genieclust.plots">Plotting Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-genieclust.tools">Other</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">genieclust</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Python Package <cite>genieclust</cite> Reference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/python_api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="python-package-genieclust-reference">
<h1>Python Package <cite>genieclust</cite> Reference<a class="headerlink" href="#python-package-genieclust-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="genie">
<h2>Genie<a class="headerlink" href="#genie" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="genieclust.Genie">
<em class="property">class </em><code class="sig-prename descclassname">genieclust.</code><code class="sig-name descname">Genie</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_clusters</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">gini_threshold</span><span class="o">=</span><span class="default_value">0.3</span></em>, <em class="sig-param"><span class="n">M</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">affinity</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">compute_full_tree</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">compute_all_cuts</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">postprocess</span><span class="o">=</span><span class="default_value">'boundary'</span></em>, <em class="sig-param"><span class="n">exact</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">cast_float32</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">use_mlpack</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.Genie" title="Permalink to this definition">¶</a></dt>
<dd><p>The Genie++ hierarchical clustering algorithm with optional smoothing and
noise point detection (for M&gt;1).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_clusters</strong><span class="classifier">int</span></dt><dd><p>Number of clusters to detect &gt;= 0. Note that depending on the dataset
and approximations used (see parameter <cite>exact</cite>), the actual
partition cardinality can be smaller.
n_clusters==1 can act as a noise point/outlier detector (if <cite>M</cite>&gt;1
and postprocess is not “all”).
n_clusters==0 computes the whole dendrogram but doesn’t generate
any particular cuts.</p>
</dd>
<dt><strong>gini_threshold</strong><span class="classifier">float</span></dt><dd><p>The threshold for the Genie correction in [0,1], i.e.,
the Gini index of the cluster size distribution.
Threshold of 1.0 disables the correction.
Low thresholds highly penalise the formation of small clusters.</p>
</dd>
<dt><strong>M</strong><span class="classifier">int</span></dt><dd><p>Smoothing factor. M=1 gives the original Genie algorithm.</p>
</dd>
<dt><strong>affinity</strong><span class="classifier">{‘euclidean’, ‘l2’, ‘manhattan’, ‘l1’, ‘cityblock’, ‘cosine’, ‘precomputed’}</span></dt><dd><p>Metric used to compute the linkage. One of: “euclidean” (synonym: “l2”),
“manhattan” (a.k.a. “l1” and “cityblock”), “cosine” or “precomputed”.
If “precomputed”, a n_samples*(n_samples-1)/2 distance vector
or a square-form distance
matrix is needed on input (argument X) for the fit() method,
see <cite>scipy.spatial.distance.pdist()</cite> or
<cite>scipy.spatial.distance.squareform()</cite>, amongst others.</p>
</dd>
<dt><strong>compute_full_tree</strong><span class="classifier">bool</span></dt><dd><p>If True, only a partial hierarchy is determined so that
at most n_clusters are generated. Saves some time if you think you know
how many clusters are there, but are you <em>really</em> sure about that?</p>
</dd>
<dt><strong>compute_all_cuts</strong><span class="classifier">bool</span></dt><dd><p>If True, n_clusters-partition and all the more coarse-grained
ones will be determined; in such a case, the <a href="#id17"><span class="problematic" id="id18">labels_</span></a> attribute
will be a matrix</p>
</dd>
<dt><strong>postprocess</strong><span class="classifier">{‘boundary’, ‘none’, ‘all’}</span></dt><dd><p>In effect only if M&gt;1. By default, only “boundary” points are merged
with their nearest “core” points (A point is a boundary point if it is
a noise point and it’s amongst its adjacent vertex’s
M-1 nearest neighbours). To force a classical
n_clusters-partition of a data set (with no notion of noise),
choose “all”.</p>
</dd>
<dt><strong>exact</strong><span class="classifier">bool</span></dt><dd><p>TODO: Not yet implemented.
If False, the minimum spanning tree is approximated
based on the nearest neighbours graph. Finding nearest neighbours
in low dimensional spaces is usually fast. Otherwise,
the algorithm will need to inspect all pairwise distances,
which gives the time complexity of O(n_samples*n_samples*n_features).</p>
</dd>
<dt><strong>cast_float32</strong><span class="classifier">bool, default=True</span></dt><dd><p>Allow casting input data to a float32 dense matrix
(for efficiency reasons; decreases the run-time ~2x times
at a cost of greater memory usage).
TODO: Note that some nearest neighbour search
methods require float32 data anyway.
TODO: Might be a problem if the input matrix is sparse, but
we don’t support this yet.</p>
</dd>
<dt><strong>use_mlpack</strong><span class="classifier">bool or “auto”, default=”auto”</span></dt><dd><p>Use mlpack.emst() for computing the Euclidean minimum spanning tree?
Might be faster for lower-dimensional spaces. As the name suggests,
only affinity=’euclidean’ is supported (and M=1).
By default, we rely on mlpack if it’s installed and n_features &lt;= 6.
Otherwise, we use our own implementation of a parallelised version
of Prim’s algorithm (environment variable <cite>OMP_NUM_THREADS</cite> controls
the number of threads used).</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool, default=False</span></dt><dd><p>Whether to print diagnostic messages and progress information on stderr.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>A reimplementation of Genie - a robust and outlier resistant
clustering algorithm <a class="reference internal" href="#r6050e59d50fb-1" id="id1">[1]</a>,
originally published as an R package <cite>genie</cite>.</p>
<p>The Genie algorithm is based on a minimum spanning tree (MST) of the
pairwise distance graph of a given point set.
Just like single linkage, it consumes the edges
of the MST in increasing order of weights. However, it prevents
the formation of clusters of highly imbalanced sizes; once the Gini index
of the cluster size distribution raises above an assumed threshold,
a forced merge of a point group of the smallest size is performed.
Its appealing simplicity goes hand in hand with its usability;
Genie often outperforms other clustering approaches on benchmark data.</p>
<p>The clustering can also be computed with respect to the
mutual reachability distance (based, e.g., on the Euclidean metric),
which is used in the definition of the HDBSCAN* algorithm <a class="reference internal" href="#r6050e59d50fb-2" id="id2">[2]</a>.
If M&gt;1, then the mutual reachability
distance <span class="math notranslate nohighlight">\(m(i,j)\)</span> with smoothing factor M is used instead of the
chosen “raw” distance d(i,j). It holds
<span class="math notranslate nohighlight">\(m(i,j)=\max(d(i,j), c(i), c(j))\)</span>,
where <span class="math notranslate nohighlight">\(c(i)\)</span> is <span class="math notranslate nohighlight">\(d(i,k)\)</span> with k being the (M-1)-th nearest
neighbour of i. This makes “noise” and “boundary” points being
“pulled away” from each other.</p>
<p>The Genie correction together with the smoothing factor M&gt;1 (note that
M==2 corresponds to the original distance) gives a robustified version of
the HDBSCAN* algorithm that is able to detect a predefined number of
clusters. Hence it does not dependent on the DBSCAN’s somehow magical
<cite>eps</cite> parameter or the HDBSCAN’s <cite>min_cluster_size</cite> one.</p>
<p>The algorithm has O(n_samples*sqrt(n_samples)) time complexity
given a minimum spanning tree of the pairwise distance graph.
Unless we use MLPACK (or other variations, see Parameters),
our parallelised implementation of a Jarník (Prim/Dijkstra)-like method
will be called to compute an MST, which generally takes O(n^2) time
(environment variable <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> controls the number of threads).
MLPACK (see Python package <cite>mlpack</cite>) is a very fast alternative
in the case of Euclidean spaces of (very) low dimensionality and M=1.</p>
<p>Note that as in the case of all the distance-based methods,
the standardisation of the input features is definitely worth giving a try.</p>
<p>According to the algorithm’s original definition,
the resulting partition tree (dendrogram) might violate
the ultrametricity property (merges might occur at levels that
are not increasing w.r.t. a between-cluster distance).
Departures from ultrametricity are corrected by applying
<code class="docutils literal notranslate"><span class="pre">Z[:,2]</span> <span class="pre">=</span> <span class="pre">genieclust.tools.cummin(Z[::-1,2])[::-1]</span></code>.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r6050e59d50fb-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Gagolewski M., Bartoszuk M., Cena A.,
Genie: A new, fast, and outlier-resistant hierarchical clustering algorithm,
<em>Information Sciences</em> <strong>363</strong>, 2016, pp. 8-23. doi:10.1016/j.ins.2016.05.003</p>
</dd>
<dt class="label" id="r6050e59d50fb-2"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Campello R., Moulavi D., Zimek A., Sander J.,
Hierarchical density estimates for data clustering, visualization,
and outlier detection,
<em>ACM Transactions on Knowledge Discovery from Data</em> <a href="#id3"><span class="problematic" id="id4">**</span></a>10**(1), 2015, 5:1–5:51.
doi:10.1145/2733381.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>labels_</strong><span class="classifier">ndarray, shape (n_samples,) or (&lt;=n_clusters+1, n_samples), or None</span></dt><dd><p>If n_clusters==0, no <a href="#id19"><span class="problematic" id="id20">labels_</span></a> are generated (None).
If compute_all_cuts==True (the default), these are the detected
cluster labels of each point: an integer vector with labels_[i]
denoting the cluster id (in {0, …, n_clusters-1}) of the i-th object.
If M&gt;1, noise points are labelled -1 (unless taken care of in the
postprocessing stage).
Otherwise, i.e., if compute_all_cuts==False,
all partitions of cardinality down to n_clusters (if n_samples
and the number of noise points allows) are determined.
In such a case, labels_[j,i] denotes the cluster id of the i-th
point in a j-partition.
We assume that a 0- and 1- partition only distinguishes between
noise- and non-noise points, however, no postprocessing
is conducted on the 0-partition (there might be points with
labels -1 even if postprocess==”all”).</p>
</dd>
<dt><strong>n_clusters_</strong><span class="classifier">int</span></dt><dd><p>The number of clusters detected by the algorithm.
If 0, then <a href="#id21"><span class="problematic" id="id22">labels_</span></a> are not set.
Note that the actual number might be larger than the n_clusters
requested, for instance, if there are many noise points.</p>
</dd>
<dt><strong>n_samples_</strong><span class="classifier">int</span></dt><dd><p>The number of points in the fitted dataset.</p>
</dd>
<dt><strong>n_features_</strong><span class="classifier">int or None</span></dt><dd><p>The number of features in the fitted dataset.</p>
</dd>
<dt><strong>is_noise_</strong><span class="classifier">ndarray, shape (n_samples,) or None</span></dt><dd><p>is_noise_[i] is True iff the i-th point is a noise one;
For M=1, all points are no-noise ones.
Points are marked as noise even if postprocess==”all”.
Note that boundary points are also marked as noise points.</p>
</dd>
<dt><strong>children_</strong><span class="classifier">ndarray, shape (n_samples-1, 2)</span></dt><dd><p>The i-th row provides the information on the clusters merged at
the i-th iteration. Noise points are merged first, with
the corresponding <cite>distances_[i]</cite> of 0.
See the description of <cite>Z[i,0]</cite> and <cite>Z[i,1]</cite> in
<cite>scipy.cluster.hierarchy.linkage</cite>. Together with <cite>distances_</cite> and
<cite>counts_</cite>, this forms the linkage matrix that can be used for
plotting the dendrogram.
Only available if <cite>compute_full_tree==True</cite>.</p>
</dd>
<dt><strong>distances_</strong><span class="classifier">ndarray, shape (n_samples-1,)</span></dt><dd><p>Distance between the two clusters merged at the i-th iteration.
As Genie does not guarantee that that distances are
ordered increasingly (do not panic, there are some other hierarchical
clustering linkages that also violate the ultrametricity property),
these are corrected by applying
<cite>distances_ = genieclust.tools.cummin(distances_[::-1])[::-1]</cite>.
See the description of Z[i,2] in scipy.cluster.hierarchy.linkage.
Only available if compute_full_tree==True.</p>
</dd>
<dt><strong>counts_</strong><span class="classifier">ndarray, shape (n_samples-1,)</span></dt><dd><p>Number of elements in a cluster created at the i-th iteration.
See the description of Z[i,3] in scipy.cluster.hierarchy.linkage.
Only available if compute_full_tree==True.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#genieclust.Genie.fit" title="genieclust.Genie.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, y])</p></td>
<td><p>Perform clustering of the X dataset.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code>(self, X[, y])</p></td>
<td><p>Compute a k-partition and return the predicted labels, see fit().</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code>(self[, deep])</p></td>
<td><p>Get parameters for this estimator.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code>(self, **params)</p></td>
<td><p>Set the parameters of this estimator.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="genieclust.Genie.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.Genie.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform clustering of the X dataset.
See the <a href="#id23"><span class="problematic" id="id24">labels_</span></a> and <a href="#id25"><span class="problematic" id="id26">n_clusters_</span></a> attributes for the clustering result.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>X</strong><span class="classifier">ndarray, shape (n_samples, n_features)  or</span></dt><dd><blockquote>
<div><p>(n_samples*(n_samples-1)/2, ) or (n_samples, n_samples)</p>
</div></blockquote>
<p>A matrix defining n_samples in a vector space with n_features.
Hint: it might be a good idea to standardise or at least
normalise the coordinates of the
input data points by calling
<cite>X = ((X-X.mean(axis=0))/X.std(axis=None, ddof=1)).astype(np.float32, order=”C”, copy=False)</cite>
so that the dataset is centred at 0 and
has total variance of 1. This way the method becomes
translation and scale invariant.
However, if affinity=”precomputed”, then X is assumed to define
all pairwise distances between n_samples
(either in form of a distance vector or square distance matrix).</p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gic">
<h2>GIc<a class="headerlink" href="#gic" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="genieclust.GIc">
<em class="property">class </em><code class="sig-prename descclassname">genieclust.</code><code class="sig-name descname">GIc</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_clusters</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">gini_thresholds</span><span class="o">=</span><span class="default_value">[0.1, 0.3, 0.5, 0.7]</span></em>, <em class="sig-param"><span class="n">add_clusters</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">M</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">affinity</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">compute_full_tree</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">compute_all_cuts</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">postprocess</span><span class="o">=</span><span class="default_value">'boundary'</span></em>, <em class="sig-param"><span class="n">exact</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">cast_float32</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">use_mlpack</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.GIc" title="Permalink to this definition">¶</a></dt>
<dd><p>GIc (Genie+Information Criterion) Information-Theoretic
Hierarchical Clustering Algorithm</p>
<p>Computes a k-partition based on a pre-computed MST
maximising (heuristically) the information criterion [2].</p>
<p>GIc has been proposed by Anna Cena in [1] and was inspired
by Mueller’s (et al.) ITM [2] and Gagolewski’s (et al.) Genie [3]</p>
<p>GIc uses a bottom-up, agglomerative approach (as opposed to the ITM,
which follows a divisive scheme). It greedily selects for merging
a pair of clusters that maximises the information criterion [2].
By default, the initial partition is determined by considering
the intersection of clusterings found by the Genie methods with
thresholds 0.1, 0.3, 0.5 and 0.7.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_clusters</strong><span class="classifier">int &gt;= 0, default=2</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>gini_thresholds</strong><span class="classifier">float in [0,1], default=[0.1, 0.3, 0.5, 0.7]</span></dt><dd><p>The GIc algorithm optimises the information criterion
in an agglomerative way, starting from the intersection
of the clusterings returned by
Genie(n_clusters=n_clusters+add_clusters, gini_threshold=gini_thresholds[i]),
for all i=0,…,len(gini_thresholds)-1.</p>
</dd>
<dt><strong>add_clusters</strong><span class="classifier">int, default=0</span></dt><dd><p>Number of additional clusters to work with internally.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">float or None, default None</span></dt><dd><p>Dataset (intrinsic) dimensionality, if None, it will be set based on
the shape of the input matrix.</p>
</dd>
<dt><strong>M</strong><span class="classifier">int, default=1</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>affinity</strong><span class="classifier">str, default=”euclidean”</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>compute_full_tree</strong><span class="classifier">bool, default=True</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>compute_all_cuts</strong><span class="classifier">bool, default=False</span></dt><dd><p>see <cite>Genie</cite>
Note that for GIc if compute_all_cuts==True,
then the i-th cut in the hierarchy behaves as if
add_clusters=n_clusters-i. In other words, the returned cuts
will not be the same as those obtained by calling
GIc numerous times, each time with different n_clusters requested.</p>
</dd>
<dt><strong>postprocess</strong><span class="classifier">str, one of “boundary” (default), “none”, “all”</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>exact</strong><span class="classifier">bool, default=True</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>cast_float32</strong><span class="classifier">bool, default=True</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>use_mlpack</strong><span class="classifier">bool or “auto”, default=”auto”</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool, default=False</span></dt><dd><p>see <cite>Genie</cite></p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Cena A., Adaptive hierarchical clustering algorithms based on
data aggregation methods, PhD Thesis, Systems Research Institute,
Polish Academy of Sciences 2018.</p>
<p>[2] Mueller A., Nowozin S., Lampert C.H., Information Theoretic
Clustering using Minimum Spanning Trees, DAGM-OAGM 2012.</p>
<p>[3] Gagolewski M., Bartoszuk M., Cena A.,
Genie: A new, fast, and outlier-resistant hierarchical clustering algorithm,
Information Sciences 363, 2016, pp. 8-23. doi:10.1016/j.ins.2016.05.003</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>see `Genie`</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#genieclust.GIc.fit" title="genieclust.GIc.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, y])</p></td>
<td><p>Perform clustering of the X dataset.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code>(self, X[, y])</p></td>
<td><p>Compute a k-partition and return the predicted labels, see fit().</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code>(self[, deep])</p></td>
<td><p>Get parameters for this estimator.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code>(self, **params)</p></td>
<td><p>Set the parameters of this estimator.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="genieclust.GIc.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.GIc.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform clustering of the X dataset.
See the <a href="#id27"><span class="problematic" id="id28">labels_</span></a> and <a href="#id29"><span class="problematic" id="id30">n_clusters_</span></a> attributes for the clustering result.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>X</strong><span class="classifier">ndarray, shape (n_samples, n_features) or</span></dt><dd><blockquote>
<div><p>(n_samples*(n_samples-1)/2, ) or (n_samples, n_samples)</p>
</div></blockquote>
<p>see <cite>Genie.fit()</cite></p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-genieclust.compare_partitions">
<span id="external-cluster-validity-measures"></span><h2>External Cluster Validity Measures<a class="headerlink" href="#module-genieclust.compare_partitions" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="genieclust.compare_partitions.adjusted_fm_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">adjusted_fm_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.adjusted_fm_score" title="Permalink to this definition">¶</a></dt>
<dd><p>The Fowlkes-Mallows index adjusted for chance,</p>
<p>See Eqs. (2) and (4)  in (Hubert, Arabie, 1985).</p>
<p>For more details, see compare_partitions().</p>
<p>Hubert L., Arabie P., Comparing Partitions,
Journal of Classification 2(1), 1985, 193-218</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.adjusted_mi_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">adjusted_mi_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.adjusted_mi_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Adjusted mutual information score (AMI_sum)</p>
<p>For more details, see compare_partitions().</p>
<p>Vinh N.X., Epps J., Bailey J.,
Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance,
Journal of Machine Learning Research 11, 2010, pp. 2837-2854.</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure;
by definition of this index, returned values might be negative.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.adjusted_rand_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">adjusted_rand_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.adjusted_rand_score" title="Permalink to this definition">¶</a></dt>
<dd><p>The Rand index adjusted for chance.</p>
<p>For more details, see compare_partitions().</p>
<p>Hubert L., Arabie P., Comparing Partitions,
Journal of Classification 2(1), 1985, 193-218</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure;
by the very definition of this index,
returned values might be negative.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.compare_partitions">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">compare_partitions</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.compare_partitions" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the adjusted and nonadjusted Rand- and FM scores,
nonadjusted, normalised and adjusted mutual information scores,
normalised accuracy and pair sets index.</p>
<p>Let <cite>x</cite> and <cite>y</cite> represent two partitions of a set of n
elements into K and L, respectively,
nonempty and pairwise disjoint subsets,
e.g., two clusterings of a dataset with n observations
represented as label vectors. Moreover, let C be the confusion
matrix (with K rows and L columns, K&lt;=L)
corresponding to <cite>x</cite> and <cite>y</cite>, see also <cite>confusion_matrix()</cite>.
This function implements scores that quantify the similarity between <cite>x</cite>
and <cite>y</cite>. They can be used as external cluster
validity measures, i.e., in the presence of reference (ground-truth)
partitions.</p>
<p>Every index except <cite>mi_score()</cite> (which computes the mutual
information score) outputs 1 given two identical partitions.
Note that partitions are always defined up to a bijection of the set of
possible labels, e.g., (1, 1, 2, 1) and (4, 4, 2, 4)
represent the same 2-partition.</p>
<p><cite>rand_score()</cite> gives the Rand score (the <cite>probability’ of agreement
between the two partitions) and `adjusted_rand_score()</cite> is its version
corrected for chance, its expected value is 0.0 for two independent
partitions. Due to the adjustment, the resulting index might also
be negative for some inputs.</p>
<p>Similarly, <cite>fm_score()</cite> gives the Fowlkes-Mallows (FM) score
and <cite>adjusted_fm_score()</cite> is its adjusted-for-chance version.</p>
<p>Note that both the (unadjusted) Rand and FM scores are bounded from below
by $1/(K+1)$, where K is the number of clusters (unique labels
in <cite>x</cite> and <cite>y</cite>), hence their adjusted versions are preferred.</p>
<p><cite>mi_score()</cite>, <cite>adjusted_mi_score()</cite> and <cite>normalized_mi_score()</cite> are
information-theoretic scores, based on mutual information,
see the definition of $AMI_{sum}$ and $NMI_{sum}$
in (Vinh et al., 2010).</p>
<p><cite>normalized_accuracy()</cite> is defined as $(Accuracy(<a href="#id31"><span class="problematic" id="id32">C_</span></a>sigma)-1/L)/(1-1/L)$,
where $C_sigma$ is a version of the confusion matrix for given <cite>x</cite> and <cite>y</cite>,
K&lt;=L, with columns permuted based on the solution to the
Maximal Linear Sum Assignment Problem.
$Accuracy(C[sigma])$ is sometimes referred to as Purity,
e.g., in (Rendon et al. 2011).</p>
<p><cite>pair_sets_index()</cite> gives the Pair Sets Index (PSI)
adjusted for chance (Rezaei, Franti, 2016), K&lt;=L.
Pairing is based on the solution to the Linear Sum Assignment Problem
of a transformed version of the confusion matrix.</p>
<p>Hubert L., Arabie P., Comparing Partitions,
Journal of Classification 2(1), 1985, pp. 193-218, esp. Eqs. (2) and (4)</p>
<p>Rendon E., Abundez I., Arizmendi A., Quiroz E.M.,
Internal versus external cluster validation indexes,
International Journal of Computers and Communications 5(1), 2011, pp. 27-34.</p>
<p>Rezaei M., Franti P., Set matching measures for external cluster validity,
IEEE Transactions on Knowledge and Data Mining 28(8), 2016, pp. 2173-2186,
doi:10.1109/TKDE.2016.2551240</p>
<p>Vinh N.X., Epps J., Bailey J.,
Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance,
Journal of Machine Learning Research 11, 2010, pp. 2837-2854.</p>
<dl class="simple">
<dt>C<span class="classifier">ndarray, shape (xc, yc)</span></dt><dd><p>a confusion matrix, xc &lt;= yc</p>
</dd>
</dl>
<dl class="simple">
<dt>scores<span class="classifier">dict</span></dt><dd><p>a dictionary with keys ‘ar’, ‘r’, ‘afm’, ‘fm’, ‘mi’, ‘nmi’, ‘ami’,
‘nacc’ and ‘psi’,
giving the following scores: adjusted Rand, Rand,
adjusted Fowlkes-Mallows, Fowlkes-Mallows
mutual information, normalised mutual information (NMI_sum),
adjusted mutual information (AMI_sum),
normalised accuracy and pair sets index, respectively.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.compare_partitions2">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">compare_partitions2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.compare_partitions2" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls compare_partitions(confusion_matrix(x, y)).</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>scores<span class="classifier">dict</span></dt><dd><p>see compare_partitions().</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.confusion_matrix">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">confusion_matrix</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the confusion matrix (as a dense matrix)</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths</p>
</dd>
</dl>
<dl class="simple">
<dt>C<span class="classifier">ndarray, shape (xc, yc)</span></dt><dd><p>a confusion matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.fm_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">fm_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.fm_score" title="Permalink to this definition">¶</a></dt>
<dd><p>The original Fowlkes-Mallows index (not adjusted for chance)</p>
<p>The index is bounded from below 1/(K+1), where K is the number of clusters
(unique labels in <cite>x</cite> and <cite>y</cite>), hence its adjusted version are preferred,
see <cite>adjusted_fm_score()</cite>.</p>
<p>For more details, see compare_partitions().</p>
<p>Hubert L., Arabie P., Comparing Partitions,
Journal of Classification 2(1), 1985, 193-218</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.mi_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">mi_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.mi_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Mutual information score</p>
<p>For more details, see compare_partitions().</p>
<p>Vinh N.X., Epps J., Bailey J.,
Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance,
Journal of Machine Learning Research 11, 2010, pp. 2837-2854.</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.normalize_confusion_matrix">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">normalize_confusion_matrix</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.normalize_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies pivoting to a given confusion matrix.
Nice if C actually summarises clustering results,
where actual labels do not matter.</p>
<dl class="simple">
<dt>C<span class="classifier">ndarray, shape (xc,yc)</span></dt><dd><p>a c_contiguous confusion matrix</p>
</dd>
</dl>
<p>C_normalized: ndarray, shape(xc,yc)</p>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.normalized_accuracy">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">normalized_accuracy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.normalized_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalised Accuracy: (Accuracy(C[sigma])-1/L)/(1-1/L).</p>
<p>C[sigma] is a version of the confusion matrix for given x and y
with columns permuted based on the solution to the
Maximal Linear Sum Assignment Problem.</p>
<p>Accuracy(C[sigma]) is sometimes referred to as Purity,
e.g., in (Rendon et al. 2011).</p>
<p>It is assumed that y represents an L-partition
and x represents a K-partition and that K&lt;=L.</p>
<p>For more details, see compare_partitions().</p>
<p>Rendon E., Abundez I., Arizmendi A., Quiroz E.M.,
Internal versus external cluster validation indexes,
International Journal of Computers and Communications 5(1), 2011, pp. 27-34.</p>
<p>Rezaei M., Franti P., Set matching measures for external cluster validity,
IEEE Transactions on Knowledge and Data Mining 28(8), 2016, pp. 2173-2186,
doi:10.1109/TKDE.2016.2551240</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.normalized_confusion_matrix">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">normalized_confusion_matrix</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.normalized_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the confusion matrix between x and y
and applies pivoting. Nice for summarising clustering results,
where actual labels do not matter.</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>C<span class="classifier">ndarray, shape (xc, yc)</span></dt><dd><p>a confusion matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.normalized_mi_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">normalized_mi_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.normalized_mi_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalised mutual information score (NMI_sum)</p>
<p>For more details, see compare_partitions().</p>
<p>Vinh N.X., Epps J., Bailey J.,
Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance,
Journal of Machine Learning Research 11, 2010, pp. 2837-2854.</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.pair_sets_index">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">pair_sets_index</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.pair_sets_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Pair Sets Index (PSI) adjusted for chance</p>
<p>Pairing is based on the solution to the Linear Sum Assignment Problem
of a transformed version of the confusion matrix.</p>
<p>It is assumed that y represents an L-partition
and x represents a K-partition and that K&lt;=L.</p>
<p>For more details, see compare_partitions().</p>
<p>Rezaei M., Franti P., Set matching measures for external cluster validity,
IEEE Transactions on Knowledge and Data Mining 28(8), 2016, pp. 2173-2186,
doi:10.1109/TKDE.2016.2551240</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.compare_partitions.rand_score">
<code class="sig-prename descclassname">genieclust.compare_partitions.</code><code class="sig-name descname">rand_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.compare_partitions.rand_score" title="Permalink to this definition">¶</a></dt>
<dd><p>The original Rand index (not adjusted for chance),
that yields the <a href="#id7"><span class="problematic" id="id8">`</span></a>probability’ of agreement between the two partitions.</p>
<p>The index is bounded from below 1/(K+1), where K is the number of clusters
(unique labels in <cite>x</cite> and <cite>y</cite>), hence its adjusted version are preferred,
see <cite>adjusted_rand_score()</cite>.</p>
<p>For more details, see compare_partitions().</p>
<p>Hubert L., Arabie P., Comparing Partitions,
Journal of Classification 2(1), 1985, 193-218</p>
<dl class="simple">
<dt>x, y<span class="classifier">ndarray, shape (n,)</span></dt><dd><p>two small-int vectors of the same lengths, representing
two partitions of the same set</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">double</span></dt><dd><p>partition similarity measure</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-genieclust.inequity">
<span id="inequity-measures"></span><h2>Inequity Measures<a class="headerlink" href="#module-genieclust.inequity" title="Permalink to this headline">¶</a></h2>
<dl class="py attribute">
<dt id="genieclust.inequity.bonferroni_index">
<code class="sig-prename descclassname">genieclust.inequity.</code><code class="sig-name descname">bonferroni_index</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">is_sorted</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.inequity.bonferroni_index" title="Permalink to this definition">¶</a></dt>
<dd><p>The normalised Bonferroni index</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array_like, shape (n,)</span></dt><dd><p>Input vector with non-negative elements.</p>
</dd>
<dt><strong>is_sorted</strong><span class="classifier">bool</span></dt><dd><p>Indicates if x is already sorted increasingly.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>double</dt><dd><p>The value of the inequity index, a number in [0,1].</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#genieclust.inequity.gini_index" title="genieclust.inequity.gini_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gini_index</span></code></a></dt><dd><p>The normalised Gini index</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The normalised Bonferroni <a class="reference internal" href="#r7a1a970a7245-1" id="id9">[1]</a> index is given by:</p>
<div class="math notranslate nohighlight">
\[B(x_1,\dots,x_n) = \frac{
\sum_{i=1}^{n}  \left( n-\sum_{j=1}^i \frac{n}{n-j+1} \right) x_{\sigma(n-i+1)}
}{
(n-1) \sum_{i=1}^n x_i
},\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma\)</span> is an ordering permutation of <span class="math notranslate nohighlight">\((x_1,\dots,x_n)\)</span>.</p>
<p>Time complexity: <span class="math notranslate nohighlight">\(O(n)\)</span> for sorted data.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r7a1a970a7245-1"><span class="brackets"><a class="fn-backref" href="#id9">1</a></span></dt>
<dd><p>Bonferroni C., <em>Elementi di Statistica Generale</em>, Libreria Seber,
Firenze, 1930.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="genieclust.inequity.gini_index">
<code class="sig-prename descclassname">genieclust.inequity.</code><code class="sig-name descname">gini_index</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">is_sorted</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.inequity.gini_index" title="Permalink to this definition">¶</a></dt>
<dd><p>The normalised Gini index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array_like, shape (n,)</span></dt><dd><p>Input vector with non-negative elements.</p>
</dd>
<dt><strong>is_sorted</strong><span class="classifier">bool</span></dt><dd><p>Indicates if x is already sorted increasingly.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>double</dt><dd><p>The value of the inequity index, a number in [0,1].</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#genieclust.inequity.bonferroni_index" title="genieclust.inequity.bonferroni_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bonferroni_index</span></code></a></dt><dd><p>The normalised Bonferroni index</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The normalised Gini <a class="reference internal" href="#r2095d0fd2ecf-1" id="id11">[1]</a> index is given by:</p>
<div class="math notranslate nohighlight">
\[G(x_1,\dots,x_n) = \frac{
\sum_{i=1}^{n-1} \sum_{j=i+1}^n |x_i-x_j|
}{
(n-1) \sum_{i=1}^n x_i
}.\]</div>
<p>Time complexity is <span class="math notranslate nohighlight">\(O(n)\)</span> for sorted data; it holds:</p>
<div class="math notranslate nohighlight">
\[G(x_1,\dots,x_n) = \frac{
\sum_{i=1}^{n} (n-2i+1) x_{\sigma(n-i+1)}
}{
(n-1) \sum_{i=1}^n x_i
},\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma\)</span> is an ordering permutation of <span class="math notranslate nohighlight">\((x_1,\dots,x_n)\)</span>.</p>
<p>Both the Gini and Bonferroni indices can be used to quantify the “inequity”
of a numeric sample. They can be perceived as measures of data dispersion.
For constant vectors (perfect equity), the indices yield values of 0.
Vectors with all elements but one equal to 0 (perfect inequity),
are assigned scores of 1.
Both indices follow the Pigou-Dalton principle (are Schur-convex):
setting <span class="math notranslate nohighlight">\(x_i = x_i - h\)</span> and <span class="math notranslate nohighlight">\(x_j = x_j + h\)</span> with <span class="math notranslate nohighlight">\(h &gt; 0\)</span>
and <span class="math notranslate nohighlight">\(x_i - h \geq  x_j + h\)</span> (taking from the “rich” and giving away
to the “poor”) decreases the inequity.</p>
<p>These indices have applications in economics, amongst others.
The <cite>Genie</cite> clustering algorithm uses the Gini index as a measure
of the inequality of cluster sizes.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r2095d0fd2ecf-1"><span class="brackets"><a class="fn-backref" href="#id11">1</a></span></dt>
<dd><p>Gini C., <em>Variabilita e Mutabilita</em>,
Tipografia di Paolo Cuppini, Bologna, 1912.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-genieclust.plots">
<span id="plotting-functions"></span><h2>Plotting Functions<a class="headerlink" href="#module-genieclust.plots" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="genieclust.plots.plot_scatter">
<code class="sig-prename descclassname">genieclust.plots.</code><code class="sig-name descname">plot_scatter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.plots.plot_scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Draws a scatter plot.</p>
<p>Unlike in <cite>matplitlib.pyplot.scatter()</cite>, all points in <cite>X</cite>
corresponding to <cite>labels == i</cite> are always drawn in the same way,
no matter the <cite>max(labels)</cite>.</p>
<dl class="simple">
<dt>X<span class="classifier">ndarray, shape (n, 2) or ndarray, shape (n,)</span></dt><dd><p>A two-column matrix giving the x and y coordinates of the points.
Optionally, these can be given by both X and y.</p>
</dd>
<dt>y<span class="classifier">None or ndarray, shape (n,)</span></dt><dd><p>y coordinates in the case of X being a vector</p>
</dd>
<dt>labels<span class="classifier">ndarray, shape (n,) or None</span></dt><dd><p>A vector of integer labels corresponding to each point in <cite>X</cite>,
giving its plot style.</p>
</dd>
<dt><a href="#id13"><span class="problematic" id="id14">**</span></a>kwargs<span class="classifier">Collection properties</span></dt><dd><p>Further arguments to <cite>matplotlib.pyplot.scatter()</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="genieclust.plots.plot_segments">
<code class="sig-prename descclassname">genieclust.plots.</code><code class="sig-name descname">plot_segments</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">pairs</span></em>, <em class="sig-param"><span class="n">style</span><span class="o">=</span><span class="default_value">'k-'</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.plots.plot_segments" title="Permalink to this definition">¶</a></dt>
<dd><p>Draws a set of disjoint line segments given by
(X[pairs[i,0],0], X[pairs[i,0],1])–(X[pairs[i,1],0], X[pairs[i,1],1]),
i = 0, …., pairs.shape[0]-1.</p>
<p>Calls <cite>matplotlib.pyplot.plot()</cite> once =&gt; it’s fast.</p>
<dl class="simple">
<dt>X<span class="classifier">ndarray, shape (n, 2)</span></dt><dd><p>A two-column matrix giving the X and Y coordinates of the points.</p>
</dd>
<dt>pairs<span class="classifier">ndarray, shape (m, 2)</span></dt><dd><p>A two-column matrix, giving the pairs of indices
defining the line segments.</p>
</dd>
</dl>
<p>style: see <cite>matplotlib.pyplot.plot()</cite></p>
<dl class="simple">
<dt><a href="#id15"><span class="problematic" id="id16">**</span></a>kwargs<span class="classifier">Collection properties</span></dt><dd><p>Further arguments to <cite>matplotlib.pyplot.plot()</cite>.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-genieclust.tools">
<span id="other"></span><h2>Other<a class="headerlink" href="#module-genieclust.tools" title="Permalink to this headline">¶</a></h2>
<dl class="py attribute">
<dt id="genieclust.tools.argkmin">
<code class="sig-prename descclassname">genieclust.tools.</code><code class="sig-name descname">argkmin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.tools.argkmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index of the (k-1)-th smallest value in an array x</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array_like</span></dt><dd><p>A c_contiguous vector of ints, floats, or doubles of length n.</p>
</dd>
<dt><strong>k</strong><span class="classifier">int</span></dt><dd><p>an integer in {0,…,n-1}, preferably small</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>int</dt><dd><p>The index where the (k-1)-th smallest value in x is located.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>It holds <code class="docutils literal notranslate"><span class="pre">argkmin(x,</span> <span class="pre">0)</span> <span class="pre">==</span> <span class="pre">argmin(x)</span></code>, or, more generally,
<code class="docutils literal notranslate"><span class="pre">argkmin(x,</span> <span class="pre">k)</span> <span class="pre">==</span> <span class="pre">np.argsort(x)[k]</span></code>.</p>
<p>Run time is O(nk) and working mem is O(k). An insertion sort-like
scheme is used to locate the order statistics.
In practice, it’s very fast for small k and randomly ordered
or almost sorted (increasingly) data.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 51%" />
<col style="width: 22%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Example timings</p></th>
<th class="head"><p><cite>argkmin(x, k)</cite></p></th>
<th class="head"><p><cite>np.argsort(x)[k]</cite></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(ascending)  n= 100000000, k=   1:</p></td>
<td><p>0.060s</p></td>
<td><p>4.388s</p></td>
</tr>
<tr class="row-odd"><td><p>(descending)</p></td>
<td><p>0.168s</p></td>
<td><p>7.329s</p></td>
</tr>
<tr class="row-even"><td><p>(random)</p></td>
<td><p>0.073s</p></td>
<td><p>26.673s</p></td>
</tr>
<tr class="row-odd"><td><p>(ascending)  n= 100000000, k=   5:</p></td>
<td><p>0.060s</p></td>
<td><p>4.403s</p></td>
</tr>
<tr class="row-even"><td><p>(descending)</p></td>
<td><p>0.505s</p></td>
<td><p>7.414s</p></td>
</tr>
<tr class="row-odd"><td><p>(random)</p></td>
<td><p>0.072s</p></td>
<td><p>26.447s</p></td>
</tr>
<tr class="row-even"><td><p>(ascending)  n= 100000000, k= 100:</p></td>
<td><p>0.061s</p></td>
<td><p>4.390s</p></td>
</tr>
<tr class="row-odd"><td><p>(descending)</p></td>
<td><p>8.007s</p></td>
<td><p>7.639s</p></td>
</tr>
<tr class="row-even"><td><p>(random)</p></td>
<td><p>0.075s</p></td>
<td><p>27.299s</p></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="genieclust.tools.argsort">
<code class="sig-prename descclassname">genieclust.tools.</code><code class="sig-name descname">argsort</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">stable</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.tools.argsort" title="Permalink to this definition">¶</a></dt>
<dd><p>Finds the[R7181d5aab013-1]_ ordering permutation of a c_contiguous array x</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array_like</span></dt><dd><p>A c_contiguous vector of ints, floats, or doubles of length n.</p>
</dd>
<dt><strong>stable</strong><span class="classifier">bool</span></dt><dd><p>Should a stable (a bit slower) sorting algorithm be used?</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>ndarray</dt><dd><p>The ordering permutation.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<dl class="citation">
<dt class="label" id="r7181d5aab013-1"><span class="brackets">1</span></dt>
<dd><p>The ordering permutation is uniquely defined provided that
<cite>stable</cite> is True, otherwise it’s <em>an</em> ordering permutation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="genieclust.tools.cummax">
<code class="sig-prename descclassname">genieclust.tools.</code><code class="sig-name descname">cummax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.tools.cummax" title="Permalink to this definition">¶</a></dt>
<dd><p>Cumulative maximum</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array_like</span></dt><dd><p>A c_contiguous vector of ints, floats, or doubles of length n.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>ndarray, shape (n,)</dt><dd><p>It holds <code class="docutils literal notranslate"><span class="pre">ret[i]</span> <span class="pre">=</span> <span class="pre">max(x[:i])</span></code> for all <code class="docutils literal notranslate"><span class="pre">i</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="genieclust.tools.cummin">
<code class="sig-prename descclassname">genieclust.tools.</code><code class="sig-name descname">cummin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#genieclust.tools.cummin" title="Permalink to this definition">¶</a></dt>
<dd><p>Cumulative minimum</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array_like</span></dt><dd><p>A c_contiguous vector of ints, floats, or doubles of length n.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>ndarray, shape (n,)</dt><dd><p>It holds <code class="docutils literal notranslate"><span class="pre">ret[i]</span> <span class="pre">=</span> <span class="pre">min(x[:i])</span></code> for all <code class="docutils literal notranslate"><span class="pre">i</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="genieclust: Fast and Robust Hierarchical Clustering with Noise Point Detection" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Marek Gagolewski

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>