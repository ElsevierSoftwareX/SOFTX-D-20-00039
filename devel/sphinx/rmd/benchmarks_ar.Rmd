# Benchmarks

Let's evaluate the usefulness of different clustering algorithms,
including the most and the to-be-most popular ones such
as agglomerative hierarchical methods
(single, complete, average, Ward linkage,
implemented in the [fastcluster](http://www.danifold.net/fastcluster.html)
package), k-means, Gaussian mixtures,
BIRCH, spectral (implemented in [scikit-learn](https://scikit-learn.org/),
[ITM](https://github.com/amueller/information-theoretic-mst) [^10],
and Genie [^5].

The [Benchmark Suite for Clustering Algorithms - Version 1](https://github.com/gagolews/clustering_benchmarks_v1)
[^6], aggregates datasets from various sources, including, but
not limited to\ [^2],\ [^3],\ [^4],\ [^7],\ [^9],\ [^11].

Ground-truth/reference label vectors are provided alongside each dataset.
They define the desired number of clusters, K.
Cluster similarity measures (here we will use the adjusted Rand index,
see\ [^8]) can be used to compare the agreement between the reference and the generated partition.
However, as there might be multiple equally valid/plausible/useful partitions
(see also [^1] for discussion),
the outputs generated by a single algorithm should be evaluated against
all the available reference labellings and the maximal similarity score
is reported.



```{r hello,echo=FALSE}
library("reticulate")
library("knitr")
use_python("/usr/bin/python3")
library("knitr")

opts_chunk$set(
    fig.path='figures_benchmarks_ar/',
    cache.path='cache_benchmarks_ar/',
    autodep=FALSE,
    cache=FALSE
)
```


```{python imports,results="hide",echo=FALSE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os.path, glob, re
#from natsort import natsorted
#import genieclust
import sklearn.metrics
import seaborn as sns
np.set_printoptions(precision=3, threshold=50, edgeitems=50)
pd.set_option("min_rows", 200)
plt.style.use("seaborn-whitegrid")
#plt.rcParams["figure.figsize"] = (8,4)



#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````

similarity_measure = "ar"


# Load results file:
res  = pd.read_csv("v1-scores.csv")
dims = pd.read_csv("v1-dims.csv")

# ari, afm can be negative --> replace negative indexes with 0.0
res.iloc[:,4:] = res.iloc[:,4:].transform(lambda x: np.maximum(x, 0.0), axis=1)

res = res.loc[res.method.isin([
    "Genie_G0.1", "Genie_G0.3", "Genie_G0.5", "Genie_G1.0", "ITM",
    "fastcluster_complete", "fastcluster_centroid", "fastcluster_average",
    "fastcluster_ward", "sklearn_kmeans", "sklearn_gm", "sklearn_spectral_Arbf_G5",
    "sklearn_birch_T0.01_BF100"]), :]

res["method"] = res["method"].map({
    "Genie_G0.1": "Genie_0.1",
    "Genie_G0.3": "Genie_0.3",
    "Genie_G1.0": "single",
    "ITM": "ITM",
    "Genie_G0.5": "Genie_0.5",
    "fastcluster_complete": "complete",
    "fastcluster_average": "average",
    "fastcluster_centroid": "centroid",
    "fastcluster_ward": "ward",
    "sklearn_kmeans": "kmeans",
    "sklearn_gm": "gauss_mix",
    "sklearn_spectral_Arbf_G5": "spectral_rbf_5",
    "sklearn_birch_T0.01_BF100": "birch_0.01",
    })


#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
# Let's first inspect the missing clusterings.

# TODO: identify missing cases, assign them with scores == 0.0


# how_many_labels_true_considered = \
#     res.groupby(["dataset", "method"]).ar.count().\
#         unstack(fill_value=0).stack().rename("how_many_labels").reset_index()
# num_labels_true = res.loc[:,["dataset", "labels"]].drop_duplicates().\
#     groupby(["dataset"]).labels.count()
#
#
# # Number of missing comparisons:
# (
#     num_labels_true-
#     how_many_labels_true_considered.
#         set_index(["dataset", "method"]).iloc[:,0]
# ).rename("num_missing").reset_index().groupby("method").num_missing.\
#     sum().sort_values().reset_index().query("num_missing>0")
#
# # what's missing
# _method = "sklearn_spectral_Arbf_G5"
# _all = pd.Series(res.dataset.unique())
# _all[~_all.isin(res.loc[res.method==_method, "dataset"])]


#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
```


Small Datasets
--------------

As some of the algorithms failed to converge within a reasonable time limits,
we'll first restrict ourselves to the datasets with up to 10000 observations.
As suggested in the suite description,
we will also omit the "parametric" batteries `h2mg` and `g2mg`.



Here are the boxplots of the distributions of the Adjusted Rand Index.
Note that AR=1.0 denotes a perfect agreement between the obtained
clustering and the reference one.

```{python prepare_small,results="hide",echo=FALSE}
# We suggested that "parametric" datasets g2mg, h2mg should be studied separately.
# Subset: not g2mg, not h2mg
res2 = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
res2 = res2.loc[res2.dataset.isin(dims.dataset[dims.n<=10_000]), :]

res2["dataset"] = res2["battery"] + "/" + res2["dataset"]



# For each dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res2.groupby(["dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
#res_max.head()





```{python plot_small,echo=FALSE,fig.cap="Distribution of the AR-index for each algorithm."}
# which similarity measure to report below:
__order=res_max.groupby("method")[similarity_measure].mean().sort_values().index

plt.rcParams["figure.figsize"] = (8,8)
sns.boxplot(y="method", x=similarity_measure, data=res_max,
            order=__order,
            orient="h",
            showmeans=True,
            meanprops=dict(markeredgecolor="k", marker="x"))
#plt.show()
```


```{python summary_small,echo=FALSE}
#print(res_max.groupby("method")[similarity_measure].describe().T.round(3))
```

Detailed results for each dataset:

```{python details_small,echo=FALSE,results="asis"}
print(res_max.set_index(["dataset", "method"])[similarity_measure].unstack().round(3).to_markdown())
```




Large Datasets
--------------

Here are the results for the larger datasets (70000-105600 points).

```{python prepare_large,results="hide",echo=FALSE}
# We suggested that "parametric" datasets g2mg, h2mg should be studied separately.
# Subset: not g2mg, not h2mg
res2 = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
res2 = res2.loc[res2.dataset.isin(dims.dataset[dims.n>10_000]), :]

res2["dataset"] = res2["battery"] + "/" + res2["dataset"]



# For each dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res2.groupby(["dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
#res_max.head()





```{python plot_large,echo=FALSE,fig.cap="Distribution of the AR-index for each algorithm."}
# which similarity measure to report below:
__order=res_max.groupby("method")[similarity_measure].mean().sort_values().index

plt.rcParams["figure.figsize"] = (8,8)
sns.boxplot(y="method", x=similarity_measure, data=res_max,
            order=__order,
            orient="h",
            showmeans=True,
            meanprops=dict(markeredgecolor="k", marker="x"))
#plt.show()
```


```{python summary_large,echo=FALSE}
#print(res_max.groupby("method")[similarity_measure].describe().T.round(3))
```

Detailed results for each dataset:

```{python details_large,echo=FALSE,results="asis"}
print(res_max.set_index(["dataset", "method"])[similarity_measure].unstack().round(3).to_markdown())
```

References
----------

[^1]: Dasgupta S., Ng V., Single Data, Multiple Clusterings,
In: *Proc. NIPS Workshop Clustering: Science or Art? Towards
Principled Approaches*, 2009. Available at https://clusteringtheory.org.

[^2]: Dua D., Graff C., *UCI Machine Learning Repository*
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science, 2019.

[^3]: Fränti P., Mariescu-Istodor R., Zhong C., XNN graph,
In: *Proc. IAPR Joint Int. Workshop on Structural, Syntactic,
and Statistical Pattern Recognition*, Merida, Mexico,
Lecture Notes in Computer Science 10029, 2016, 207-217.
doi:10.1007/978-3-319-49055-7_19.

[^4]: Fränti P., Sieranoja S., K-means properties on six clustering benchmark
datasets, *Applied Intelligence* 48, 2018, 4743-4759.
doi:10.1007/s10489-018-1238-7.

[^5]: Gagolewski M., Bartoszuk M., Cena A.,  Genie: A new, fast and
outlier-resistant hierarchical clustering algorithm,
*Information Sciences* 363, 2016, 8-23. doi:10.1016/j.ins.2016.05.003.

[^6]: Gagolewski M., Cena A. (Eds.), *Benchmark Suite for Clustering Algorithms — Version 1*,
2020, https://github.com/gagolews/clustering_benchmarks_v1. doi:10.5281/zenodo.3815066.

[^7]: Graves D., Pedrycz W., Kernel-based fuzzy clustering and fuzzy clustering:
A comparative experimental study, *Fuzzy Sets and Systems* 161(4), 2010,
522-543. doi:10.1016/j.fss.2009.10.021.

[^8]: Hubert L., Arabie P., Comparing Partitions, *Journal of Classification* 2(1),
1985, 193-218.

[^9]: Karypis G., Han E.H., Kumar V., CHAMELEON: A hierarchical clustering
algorithm using dynamic modeling, *IEEE Transactions on Computers* 32(8),
1999, 68-75. doi:10.1109/2.781637.

[^10]: Mueller A., Nowozin S., Lampert C.H.,
Information Theoretic Clustering using Minimum Spanning Trees,
*DAGM-OAGM*, 2012.

[^11]: Ultsch A., Clustering with SOM: U*C,
In: *Proc. Workshop on Self-Organizing Maps*, Paris, France, 2005, 75-82.
