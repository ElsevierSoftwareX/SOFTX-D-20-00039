Timings
=======


**TODO: under construction.**

We consider the "larger" datasets (70000-105600 observations,
see section on Benchmark Results for discussion)
from the
`Benchmark Suite for Clustering Algorithms — Version 1 <https://github.com/gagolews/clustering_benchmarks_v1>`_ [1]_. Features with variance of 0 were removed,
datasets were centred at **0** and scaled so that they have total variance is 1.
Tiny bit of Gaussian noise was added to each observation.
Clustering is performed with respect to the Euclidean distance.

Comparison with k-means from `scikit-learn <https://scikit-learn.org/>`_ version 0.23.1
(`sklearn.cluster.KMeans`)
for different number of threads (default is to use all available threads).
Note that `n_init` defaults to 10.

`mlpack`'s emst is used for low-dimensional spaces.

Timings were performed on a PC running GNU/Linux 5.4.0-40-generic #44-Ubuntu SMP x86_64 kernel with an Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz (12M cache, 6 cores, 12 threads)
and total memory of 16242084 kB.




<<timings-imports,results="hidden",echo=False>>=
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os.path, glob, re
import scipy.stats
#from natsort import natsorted
#import genieclust
import sklearn.metrics
import seaborn as sns
import pweave
from tabulate import tabulate
np.set_printoptions(precision=3, threshold=50, edgeitems=50)
pd.set_option("min_rows", 200)
pd.set_option("max_columns", 20)
#pd.set_option("display.width", 200)
plt.style.use("bmh")
plt.rcParams.update({
    'font.size': 9,
    'font.family': 'sans-serif',
    'font.sans-serif': ['Ubuntu Condensed', 'Alegreya', 'Alegreya Sans']})


res  = pd.read_csv("v1-timings.csv") # see timings.py
dims = pd.read_csv("v1-dims.csv")
dims["dataset"] = dims["battery"]+"/"+dims["dataset"]
dims = dims.loc[:,"dataset":]

#res = res.loc[res.method.isin([
#    "Genie_G0.1", "Genie_G0.3", "Genie_G0.5", "Genie_G1.0", "ITM",
#    "fastcluster_complete", "fastcluster_centroid", "fastcluster_average",
#    "fastcluster_ward", "sklearn_kmeans", "sklearn_gm", "sklearn_spectral_Arbf_G5",
#    "sklearn_birch_T0.01_BF100"]), :]
#


#res["method"] = res["method"].map({
#    "Genie_G0.1": "Genie_0.1",
#    "Genie_G0.3": "Genie_0.3",
#    "Genie_G1.0": "single",
#    "ITM": "ITM",
#    "Genie_G0.5": "Genie_0.5",
#    "fastcluster_complete": "complete",
#    "fastcluster_average": "average",
#    "fastcluster_centroid": "centroid",
#    "fastcluster_ward": "ward",
#    "sklearn_kmeans": "kmeans",
#    "sklearn_gm": "gauss_mix",
#    "sklearn_spectral_Arbf_G5": "spectral_rbf_5",
#    "sklearn_birch_T0.01_BF100": "birch_0.01",
#    })
@

Minimum of 3 run times (**TODO**: except mnist/digits and mnist/fashion):

<<timings-get-min,results="hidden",echo=False>>=
res2 = res.loc[(res.n_threads>0), "dataset":]
res2 = res2.loc[res.dataset.isin(["mnist/digits", "mnist/fashion",
    "sipu/worms_2", "sipu/worms_64", "sipu/birch1", "sipu/birch2"]), :]
res2 = res2.groupby(["dataset", "method", "n_clusters", "n_threads"]).\
    elapsed_time.min().reset_index()
res2 = pd.merge(res2, dims, on="dataset")
# what's missing:
# pd.set_option("display.width", 200)
# res.groupby(["dataset", "method", "n_clusters", "n_threads"]).size().unstack([3,2])
@




Results (in seconds) as a function of the number of clusters to detect (6 threads):

<<timings-summary,results="rst",echo=False>>=
_dat = res2.loc[res2.n_threads==6,["dataset","n", "d", "method","n_clusters","elapsed_time"]].\
set_index(["dataset","n", "d", "method","n_clusters"]).unstack().reset_index().round(2)
_dat.columns = [l0 if not l1 else l1 for l0, l1 in _dat.columns]
which_repeated = (_dat.dataset.shift(1) == _dat.dataset)
_dat.loc[which_repeated, "dataset"] = ""
_dat["n"] = _dat["n"].astype(str)
_dat.loc[which_repeated, "n"] = ""
_dat["d"] = _dat["d"].astype(str)
_dat.loc[which_repeated, "d"] = ""
print(tabulate(_dat, _dat.columns, tablefmt="rst", showindex=False), "\n\n")
@



Number of threads (jobs):

<<timings-plot,results="hidden",echo=False,caption="Timings [s] as a function of the number of clusters and threads.">>=
dataset="mnist/digits"
sns.lineplot(x="n_clusters", y="elapsed_time", hue="method", style="n_threads", data=res2.loc[res2.dataset==dataset,:], markers=True)
plt.title("%s (n=%d, d=%d)" %(dataset, dims.loc[dims.dataset==dataset,"n"], dims.loc[dims.dataset==dataset,"d"]))
plt.xscale("log")
#plt.yscale("log")
plt.ylim(0, 2000)
plt.show()
@

**TODO**: for Genie, the number of clusters to extract does not affect
the run-time. Genie itself has :math:`O(n \sqrt{n})` time complexity.

**TODO**: mention cache, show timings — once we determine the MST,
we can play with different `gini_threshold`\ s for "free".


The effect of the curse of dimensionality is clearly visible -- clustering
in very low-dimensional Euclidean spaces is extremely fast.
Then the timings become grow linearly as a function of dimensionality, `d` --
:math:`O(d n^2)` time is needed.

Importantly, the algorithm only needs :math:`O(n)` memory.


TODO: mention extreme clustering





Timings as a Function of `n` and `d`
------------------------------------

Synthetic datasets being two Gaussian blobs, each of size `n/2`
(with i.i.d. coordinates), in a `d`-dimensional space.

Medians of 1,3, or 10 timings (depending on the dataset size), in seconds,
on 6 threads:

<<g2mg-summary,results="rst",echo=False>>=
g2mg  = pd.read_csv("v1-g2mg.csv") # see timings_g2mg.py
# What's missing:
# g2mg.loc[g2mg.n_threads>0,:].groupby(["method", "n", "d"])[["elapsed_time"]].size().unstack(0)
# g2mg.loc[g2mg.n_threads>0,:].groupby(["method", "n", "d"])[["elapsed_time"]].agg(scipy.stats.variation).unstack(0)
_dat = g2mg.loc[g2mg.method.isin(["Genie_0.3_approx", "Genie_0.3_nomlpack", "Genie_0.3_mlpack"]) & (g2mg.n.isin([10_000, 50_000, 100_000, 500_000, 1_000_000])),["method","n","d","elapsed_time"]].groupby(["method","n","d"]).median().reset_index()
_dat = _dat.set_index(["method", "d", "n"]).unstack().round(2).reset_index()
_dat.columns = [l0 if not l1 else l1 for l0, l1 in _dat.columns]
which_repeated = (_dat.method.shift(1) == _dat.method)
_dat.loc[which_repeated, "method"] = ""
print(tabulate(_dat, _dat.columns, tablefmt="rst", showindex=False), "\n\n")
@

By default, `mlpack_enabled` is ``"auto"``, which translates
to ``True`` if the requested metric is Euclidean, `mlpack` Python package is available
and `d` is not greater than 6.


<<g2mg-plot,results="hidden",echo=False,caption="Timings [s] as a function of the dataset size and dimensionality — problem sizes that can be solved within a coffee-break.">>=
_dat = g2mg.loc[g2mg.method.isin(["Genie_0.3_approx", "Genie_0.3_nomlpack", "Genie_0.3_mlpack"])&(g2mg.d>10),["method","n","d","elapsed_time"]].groupby(["method","n","d"]).median().reset_index()
sns.lineplot(x="n", y="elapsed_time", hue="method", style="d", data=_dat, markers=True)
#plt.yscale("log")
plt.xscale("log")
plt.ylim(0, 600)
plt.show()
@


**TODO:** conclusions




Benchmarking the Approximate Version
------------------------------------

**TODO:** move to the appendix

**TODO:**
On each benchmark dataset ("small" and "large" altogether)
we have fired 10 runs of the approximate Genie method ``exact=False``.

<<approx-diffs-load,results="hidden",echo=False>>=
# Load results file:
res = pd.read_csv("v1-scores-approx.csv")
# ari, afm can be negative --> replace negative indexes with 0.0
res.iloc[:,4:] = res.iloc[:,4:].transform(lambda x: np.maximum(x, 0.0), axis=1)


# I suggest that g2mg, h2mg and all datasets with n>10000 should
# be studied separately.
# Subset: not g2mg, not h2mg
res = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
dims = pd.read_csv("v1-dims.csv")
#res = res.loc[~res.dataset.isin(dims.dataset[dims.n>10_000]), :]



#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````


# For each battery, dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res.groupby(["battery", "dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
params = res_max.method.str.extract("^(Genie_([^_]+)(?:_approx)?)(?:_s([0-9]+))?$")
params.columns = ["method", "gini_threshold", "run"]
res_max = pd.concat((res_max.drop("method", axis=1), params), axis=1)
res_max["dataset"] = res_max["battery"] + "/" + res_max["dataset"]
res_max = res_max.iloc[:, 1:]
@



A complete list of datasets and thresholds where the approximate method
yields a difference in AR-indices (AR index of one of 10 runs of the approximate
method runs minus the AR index for the exact method) detectable at 2 digits of precision:

<<approx-diffs,results="rst",echo=False>>=
# which similarity measure to report below:
similarity_measure = "ar"

diffs = \
res_max.query('run==run').set_index(["dataset", "gini_threshold","run"])[similarity_measure]-\
res_max.query('run!=run').set_index(["dataset", "gini_threshold"])[similarity_measure].rename(similarity_measure)
diffs = diffs.reset_index()
diffs_stats = diffs.groupby(["dataset", "gini_threshold"])[similarity_measure].describe().reset_index()
_dat = diffs_stats.loc[(np.abs(diffs_stats["min"])>=0.0095)|(np.abs(diffs_stats["max"])>=0.0095),:].round(2)
#_dat = _dat.drop("count", axis=1)
which_repeated = (_dat.dataset.shift(1) == _dat.dataset)
_dat.loc[which_repeated, "dataset"] = ""
print(tabulate(_dat, _dat.columns, tablefmt="rst", showindex=False), "\n\n")
@


The only noteworthy  difference is for the ``sipu/birch`` dataset
where we observe that the approximate method generates worse results
(although recall that `gini_threshold` of 1 corresponds to the single linkage method).
Interestingly, for ``sipu/worms2``, `gini_threshold` of 0.5 behaves much better.



Descriptive statistics for AR-indices (for the approximate method we choose
the median AR in each of the 10 runs):

<<approx-ar,results="rst",echo=False>>=
_dat = res_max.groupby(["dataset", "method"])[similarity_measure].\
    median().reset_index().groupby(["method"]).describe().\
    round(3).reset_index()
_dat.columns = [l0 if not l1 else l1 for l0, l1 in _dat.columns]

_dat.method

#which_repeated = (_dat.gini_threshold.shift(1) == _dat.gini_threshold)
#_dat.loc[which_repeated, "gini_threshold"] = ""
#_dat = _dat.drop("count", axis=1)
print(tabulate(_dat, _dat.columns, tablefmt="rst", showindex=False), "\n\n")
@



References
----------

.. [1]
    Gagolewski M., Cena A. (Eds.), *Benchmark Suite for Clustering Algorithms — Version 1*,
    2020. https://github.com/gagolews/clustering_benchmarks_v1. doi:10.5281/zenodo.3815066.
